{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: lauritto\n",
      "P√°gina 1: https://www.analisisdigital.com.ar/busqueda?search_api_fulltext=lauritto&page=0\n",
      "P√°gina 2: https://www.analisisdigital.com.ar/busqueda?search_api_fulltext=lauritto&page=1\n",
      "P√°gina 3: https://www.analisisdigital.com.ar/busqueda?search_api_fulltext=lauritto&page=2\n",
      "Buscando: paran√°\n",
      "P√°gina 1: https://www.analisisdigital.com.ar/busqueda?search_api_fulltext=paran√°&page=0\n",
      "P√°gina 2: https://www.analisisdigital.com.ar/busqueda?search_api_fulltext=paran√°&page=1\n",
      "P√°gina 3: https://www.analisisdigital.com.ar/busqueda?search_api_fulltext=paran√°&page=2\n",
      "Guardado: 76 noticias en C:/Users/Lenovo/Documents/github/seguimiento-de-noticias/data/pruebas/analisisdigital_lupita.xlsx\n"
     ]
    }
   ],
   "source": [
    "# AN√ÅLISIS DIGITAL\n",
    "\n",
    "import requests, time, os, random, re\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from hashlib import md5\n",
    "import pandas as pd\n",
    "\n",
    "# ===== PALABRAS CLAVE DE PRUEBA =====\n",
    "PALABRAS_CLAVE = [\"lauritto\", \"paran√°\"]\n",
    "\n",
    "# ===== CONFIGURACI√ìN GENERAL =====\n",
    "MEDIO = \"analisisdigital\"\n",
    "MAX_PAGINAS = 3\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "URL_BASE = \"https://www.analisisdigital.com.ar/busqueda?search_api_fulltext={}&page={}\"\n",
    "SLEEP = (1.0, 2.5)\n",
    "\n",
    "OUT_PATH = r\"C:/Users/Lenovo/Documents/github/seguimiento-de-noticias/data/pruebas/analisisdigital_lupita.xlsx\"\n",
    "FECHA_CORTE_DT = datetime.today() - timedelta(days=7)\n",
    "\n",
    "# ===== PARSEO DE FECHAS =====\n",
    "def parse_fecha(texto):\n",
    "    meses = {\n",
    "        'enero':'01','febrero':'02','marzo':'03','abril':'04','mayo':'05','junio':'06',\n",
    "        'julio':'07','agosto':'08','septiembre':'09','octubre':'10','noviembre':'11','diciembre':'12'\n",
    "    }\n",
    "    try:\n",
    "        m = re.search(r'(\\d{1,2}) de ([a-z√°√©√≠√≥√∫]+) de (\\d{4})', texto.lower())\n",
    "        if m:\n",
    "            d, mes, y = int(m[1]), meses[m[2]], m[3]\n",
    "            return datetime.strptime(f\"{y}-{mes}-{d:02d}\", \"%Y-%m-%d\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ===== SCRAPEAR UNA NOTA =====\n",
    "def scrap_nota(url):\n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        titulo = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"\"\n",
    "\n",
    "        fecha_tag = soup.find('div', class_=lambda x: x and 'field--name-node-post-date' in x)\n",
    "        fecha = parse_fecha(fecha_tag.get_text(strip=True)) if fecha_tag else None\n",
    "        if not fecha or fecha < FECHA_CORTE_DT:\n",
    "            return None\n",
    "\n",
    "        cuerpo = soup.find('div', class_=lambda x: x and 'body-noticia' in x)\n",
    "        parrafos = [p.get_text(strip=True) for p in cuerpo.find_all(\"p\")] if cuerpo else []\n",
    "        texto = \"\\n\".join(parrafos)\n",
    "\n",
    "        return {\n",
    "            \"id\": md5((titulo + url).encode()).hexdigest(),\n",
    "            \"medio\": MEDIO,\n",
    "            \"fecha\": fecha.strftime(\"%Y-%m-%d\"),\n",
    "            \"titulo\": titulo,\n",
    "            \"contenido\": texto,\n",
    "            \"enlace\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scrapeando nota: {url} - {e}\")\n",
    "        return None\n",
    "\n",
    "# ===== BUSCAR POR PALABRA CLAVE =====\n",
    "def buscar_por_palabra(palabra):\n",
    "    encontrados = []\n",
    "    print(f\"Buscando: {palabra}\")\n",
    "    for pag in range(0, MAX_PAGINAS):\n",
    "        url = URL_BASE.format(palabra.replace(\" \", \"+\"), pag)\n",
    "        print(f\"P√°gina {pag+1}: {url}\")\n",
    "        try:\n",
    "            res = requests.get(url, headers=HEADERS)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            resultados = soup.find_all(\"div\", class_=\"views-row\")\n",
    "            if not resultados:\n",
    "                break\n",
    "\n",
    "            for r in resultados:\n",
    "                a = r.find(\"a\", href=True)\n",
    "                if not a:\n",
    "                    continue\n",
    "                link = \"https://www.analisisdigital.com.ar\" + a[\"href\"]\n",
    "                nota = scrap_nota(link)\n",
    "                if nota:\n",
    "                    nota[\"palabra_clave\"] = palabra\n",
    "                    encontrados.append(nota)\n",
    "\n",
    "            time.sleep(random.uniform(*SLEEP))\n",
    "        except Exception as e:\n",
    "            print(f\"Error en b√∫squeda de p√°gina {pag+1} para {palabra}: {e}\")\n",
    "            continue\n",
    "    return encontrados\n",
    "\n",
    "# ===== EJECUCI√ìN PRINCIPAL =====\n",
    "notas_totales = []\n",
    "for palabra in PALABRAS_CLAVE:\n",
    "    notas = buscar_por_palabra(palabra)\n",
    "    notas_totales.extend(notas)\n",
    "\n",
    "# ===== GUARDADO =====\n",
    "if notas_totales:\n",
    "    df = pd.DataFrame(notas_totales)\n",
    "    df = df.drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "    os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "    df.to_excel(OUT_PATH, index=False)\n",
    "    print(f\"Guardado: {len(df)} noticias en {OUT_PATH}\")\n",
    "else:\n",
    "    print(\"No se encontraron noticias.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7934970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Buscando: lauritto\n",
      "\n",
      "üìù Listado: La diputada Gaillard se reuni√≥ con Lauritto: realizar√° gestiones por el Rulo de la Ruta 39\n",
      "üëâ Contenido truncado: ‚ÄúVamos a presentar un proyecto de Resoluci√≥n a nivel nacional en el Congreso y vamos a hacer las gestiones ante Vialidad Nacional para que sea incluido el Rulo‚Äù, sostuvo la diputada nacional. Estas ge\n",
      "\n",
      "üìù Listado: Lauritto se reuni√≥ con referentes de Iglesias Evang√©licas: buscan una articulaci√≥n para abordar problem√°ticas sociales\n",
      "üëâ Contenido truncado: El encuentro dio continuidad a la reuni√≥n celebrada en octubre del a√±o pasado y reafirm√≥ el compromiso de superar los modelos asistencialistas tradicionales, apostando por soluciones sostenibles desde\n",
      "\n",
      "üìù Listado: Frigerio y Lauritto trabajaron sobre nuevos desaf√≠os y proyectos para Concepci√≥n del Uruguay\n",
      "üëâ Contenido truncado: Seg√∫n el intendente, el objetivo principal de la reuni√≥n fue explorar c√≥mo la provincia puede asistir a los municipios ante este nuevo contexto. \"Eso nos obliga a readecuar la relaci√≥n entre provincia\n",
      "\n",
      "üìù Listado: Lauritto quiere que vuelva el tren como transporte interno en Concepci√≥n del Uruguay\n",
      "üëâ Contenido truncado: Jos√© Lauritto anunci√≥ que impulsar√° su propuesta para que Concepci√≥n del Uruguay tenga un tren de cercan√≠a, que comience siendo urbano para su ciudad y que deje la posibilidad abierta para extenderse \n",
      "\n",
      "üìù Listado: Lauritto, el Secretario de Transporte y empresarios debatieron sobre la situaci√≥n del transporte urbano\n",
      "üëâ Contenido truncado: Tal y como sucede en el resto de la provincia, y de acuerdo a los responsables de las diferentes l√≠neas de transporte urbano, el precio actual del pasaje no cubre el costo del servicio prestado, inclu\n",
      "\n",
      "üìù Listado: Los intendentes del Departamento Uruguay se reunieron con Lauritto\n",
      "üëâ Contenido truncado: En la reuni√≥n, que tuvo lugar en el despacho de Lauritto, compartieron experiencias e intercambiaron ideas de gesti√≥n para propiciar el crecimiento de dichas localidades y fortalecer los lazos en nues\n",
      "\n",
      "üìù Listado: PJ: Concejal oficialista de Paran√° destac√≥ la \"construcci√≥n de acuerdos\" que proponen Romero y Lauritto\n",
      "üëâ Contenido truncado: Emanuel G√≥mez Tutau, concejal de Paran√° del bloque M√°s para Entre R√≠os, opin√≥ de la actualidad pol√≠tica del peronismo y entendi√≥ que la √∫nica manera de subsistir en un momento de debilidad es con la i\n",
      "\n",
      "üìù Listado: Frigerio y Lauritto hablaron de obras en Concepci√≥n del Uruguay\n",
      "üëâ Contenido truncado: El gobernador Rogelio Frigerio y el intendente de Concepci√≥n del Uruguay Jos√© Lauritto trataron una amplia tem√°tica de inter√©s para la cabecera del departamento Uruguay, como la adjudicaci√≥n y contrat\n",
      "\n",
      "üìù Listado: En Concordia, Lauritto hizo un llamado a la unidad y la reflexi√≥n en el peronismo\n",
      "üëâ Contenido truncado: Jos√© Lauritto particip√≥ junto a funcionarios de su gesti√≥n y concejales de un encuentro militante en el que estuvieron presentes dirigentes locales, gremialistas, representantes de instituciones depor\n",
      "\n",
      "üìù Listado: Lauritto: ‚ÄúFue un error la elecci√≥n de Kueider como Senador, no ten√≠a ning√∫n m√©rito para serlo‚Äù\n",
      "üëâ Contenido truncado: Jos√© Eduardo Lauritto, intendente de Concepci√≥n del Uruguay, analiz√≥ la situaci√≥n que vive en la actualidad el Partido Justicialista, insisti√≥ en hacer una autocr√≠tica e inst√≥ a \"construir una nueva a\n",
      "\n",
      "üìù Listado: Romero y Lauritto intercambiaron ideas de gesti√≥n\n",
      "üëâ Contenido truncado: La intendenta de Paran√° recibi√≥ a su par de La Hist√≥rica, Jos√© Lauritto, luego de la reuni√≥n realizada en Concepci√≥n del Uruguay en julio junto a los dos equipos de trabajo. La reuni√≥n de trabajo fue \n",
      "\n",
      "üìù Listado: El intendente Lauritto se reuni√≥ con autoridades de ‚ÄúLa Casa del Menor‚Äù.\n",
      "üëâ Contenido truncado: Hay instituciones que son esenciales en el entramado colectivo por la ejemplar misi√≥n que cumplen, muy especialmente aquellas que est√°n dedicadas a tender una mano a quienes m√°s lo necesitan, como suc\n",
      "üõë Fin del scroll.\n",
      "\n",
      "üîç Buscando: romero\n",
      "\n",
      "üìù Listado: La intendenta Rosario Romero entreg√≥ la Llave de la Ciudad al nuevo arzobispo de Paran√°, Ra√∫l Mart√≠n\n",
      "üëâ Contenido truncado: Durante la entrega simb√≥lica de la llave de la ciudad, Romero brind√≥ palabras de bienvenida al nuevo arzobispo de Paran√°. ‚ÄúEn Paran√° el di√°logo es una caracter√≠stica y la paz social que existe es uno \n",
      "\n",
      "üìù Listado: La intendenta Romero destac√≥ la labor de monse√±or Puiggari en su despedida del Arzobispado de Paran√°\n",
      "üëâ Contenido truncado: La audiencia institucional se realiz√≥ este jueves en la sede del Arzobispado, donde la intendenta Rosario Romero, en representaci√≥n del Municipio, le obsequi√≥ una pintura a monse√±or Juan Alberto Puigg\n",
      "\n",
      "üìù Listado: Rosario Romero resalt√≥ el inter√©s de los estudiantes en su visita a la feria Construir Futuro\n",
      "üëâ Contenido truncado: Este martes y mi√©rcoles se lleva adelante la Feria de carreras, oficios y emprendedurismo que organiza la Municipalidad en el marco del programa Construir Futuro. M√°s de dos mil estudiantes secundario\n",
      "\n",
      "üìù Listado: La Intendenta Rosario Romero se reuni√≥ con vecinales de la zona sur de Paran√°\n",
      "üëâ Contenido truncado: ‚ÄúTenemos un intercambio sobre c√≥mo ir mejorando la gesti√≥n municipal, sobre todo en servicios p√∫blicos en cada zona. Escuchamos las demandas de los vecinos, a veces hay que decir que s√≠, otras veces h\n",
      "\n",
      "üìù Listado: Rosario Romero: ‚ÄúArgentina es mucho m√°s que lo que se decide en Buenos Aires‚Äù\n",
      "üëâ Contenido truncado: ‚ÄúComo intendenta de Paran√°, ciudad capital de Entre R√≠os, comparto con mis pares la preocupaci√≥n de las ciudades del mal llamado ‚Äòinterior‚Äô del pa√≠s‚Äù, expres√≥ Romero en sus redes sociales. ‚ÄúHemos sufr\n",
      "\n",
      "üìù Listado: Rosario Romero: ‚ÄúLa cercan√≠a con la comunidad es clave para fortalecer la democracia‚Äù\n",
      "üëâ Contenido truncado: Del 21 al 23 de mayo, la conferencia convoca a compartir experiencias sobre participaci√≥n ciudadana, gesti√≥n p√∫blica y el impacto de las nuevas tecnolog√≠as en los procesos democr√°ticos. ‚ÄúEs fundamenta\n",
      "\n",
      "üìù Listado: Curvale le exigi√≥ a Romero que se solucione los problemas en el servicio de agua potable\n",
      "üëâ Contenido truncado: El dirigente radical Gustavo Curvale le envi√≥ a la intendenta Rosario Romero una carta abierta haciendo referencia a las problem√°ticas en el servicio de agua potable. ‚ÄúLos vecinos merecemos una explic\n",
      "\n",
      "üìù Listado: Romero festej√≥ ‚Äúcon esperanza‚Äù  la elecci√≥n de Prevost como nuevo Papa\n",
      "üëâ Contenido truncado: Este mi√©rcoles Robert Francis Prevost fue elegido como el nuevo papa bajo el nombre de Leon XIV a trav√©s de un c√≥nclave de cardenales que requiri√≥ de 4 votaciones para lograr 89 de votos, dos tercios \n",
      "\n",
      "üìù Listado: Rosario Romero solicit√≥ a Vialidad Nacional la reactivaci√≥n de la obra de Circunvalaci√≥n\n",
      "üëâ Contenido truncado: Romero se reuni√≥ en Buenos Aires con el gerente Operativo de Obras y Proyectos, V√≠ctor Farre, para gestionar el reinicio de obra para la finalizaci√≥n del ingreso a Paran√° por Ruta Nacional 12. El ingr\n",
      "\n",
      "üìù Listado: Rosario Romero se mostr√≥ con Kicillof\n",
      "üëâ Contenido truncado: ‚ÄúRecibimos al gobernador Axel Kicillof en Paran√°. Es un honor contar con su visita en nuestra ciudad, en tiempos donde el encuentro, el di√°logo y la construcci√≥n colectiva son m√°s necesarios que nunca\n",
      "\n",
      "üìù Listado: Rosario Romero recibi√≥ al equipo de Defensa Civil que brind√≥ asistencia en Bah√≠a Blanca\n",
      "üëâ Contenido truncado: La Intendenta de Paran√°, Rosario Romero recibi√≥ al equipo de Defensa Civil que brind√≥ asistencia en Bah√≠a Blanca, la ciudad bonaerense afectada por una gran inundaci√≥n. El personal municipal destinado\n",
      "\n",
      "üìù Listado: ‚ÄúNo nos vamos a dejar extorsionar‚Äù, advirti√≥ la intendenta Romero sobre los reclamos de los cartoneros\n",
      "üëâ Contenido truncado: ‚ÄúHemos tenido la sorpresa de que un grupo violento nucleado en el MTE (Movimiento de Trabajadores Excluidos) irrumpi√≥ en el edificio municipal esta ma√±ana exigiendo aportes que no son obligaciones del\n",
      "\n",
      "üìù Listado: La Intendenta Romero se reuni√≥ con vecinales de la zona sureste de Paran√°\n",
      "üëâ Contenido truncado: El encuentro tuvo lugar este lunes en el Sal√≥n Mariano Moreno del Concejo Deliberante. ‚ÄúEs una metodolog√≠a de trabajo que hemos optado desde el principio de la gesti√≥n con la intendenta Rosario Romero\n",
      "\n",
      "üìù Listado: Rosario Romero abri√≥ las sesiones del Concejo con un mensaje a favor del Estado facilitador y en contra de los √°nimos fundacionales\n",
      "üëâ Contenido truncado: Los puntos que guiaron su alocuci√≥n de casi una hora fueron 4. ‚ÄúLa defensa de la institucionalidad y la democracia; un Estado municipal facilitador que amerita su mejoramiento e innovaci√≥n permanente;\n",
      "\n",
      "üìù Listado: La intendenta Rosario Romero abrir√° este s√°bado las sesiones ordinarias del Concejo Deliberante\n",
      "üëâ Contenido truncado: El cuerpo deliberativo de la capital entrerriana fue convocado para este s√°bado 1¬∞ de marzo, a la primera sesi√≥n ordinaria del a√±o, a fin de dar cumplimiento a lo dispuesto en el R√©gimen Municipal par\n",
      "\n",
      "üìù Listado: La Intendenta Rosario Romero acompa√±√≥ la inauguraci√≥n del nuevo ingreso del Club Argentino Juniors\n",
      "üëâ Contenido truncado: ‚ÄúCreemos firmemente que el deporte y la cultura son motores de inclusi√≥n y promueven valores fundamentales para el desarrollo de nuestra ciudad‚Äù, se√±al√≥ Rosario Romero tras el corte de cintas que dej√≥\n",
      "\n",
      "üìù Listado: Paran√°: Rosario Romero inaugur√≥ el Sal√≥n de Usos M√∫ltiples Sue√±o Cumplido\n",
      "üëâ Contenido truncado: ‚ÄúCon poco hicimos mucho; continuamos una obra que hab√≠a empezado la gesti√≥n anterior y que hab√≠a peleado por muchos a√±os esta comunidad. Son muy pujantes y con muchas ganas, por eso le pusieron el nom\n",
      "\n",
      "üìù Listado: Rosario Romero se reuni√≥ con vecinos y formalizaron una mesa de di√°logo por el agua potable\n",
      "üëâ Contenido truncado: La intendenta de Paran√°, Rosario Romero, se reuni√≥ con vecinos de diferentes barrios de la ciudad, que se han visto afectados por los cortes de agua durante los √∫ltimos d√≠as. Junto a funcionarios muni\n",
      "\n",
      "üìù Listado: Elecciones legislativas: ‚ÄúMe gustar√≠a que la boleta peronista expresara una visi√≥n de futuro‚Äù, expres√≥ Romero\n",
      "üëâ Contenido truncado: En una entrevista, la intendenta Rosario Romero se refiri√≥ al escenario electoral de este a√±o y a las perspectivas del Peronismo, y dio su postura sobre las PASO. Consultada acerca de la posibilidad d\n",
      "\n",
      "üìù Listado: Frigerio analiz√≥ con Rosario Romero la continuidad de obras con financiamiento nacional\n",
      "üëâ Contenido truncado: El gobernador Rogelio Frigerio se reuni√≥ con la intendenta de Paran√°, Rosario Romero, para abordar temas de gesti√≥n que tienen que ver principalmente con la continuidad de obras que cuentan con financ\n",
      "\n",
      "üìù Listado: La intendenta Rosario Romero inaugur√≥ la temporada de verano en el Balneario Municipal\n",
      "üëâ Contenido truncado: En el marco de la propuesta municipal Verano en la Ciudad, la intendenta Rosario Romero concret√≥ este mi√©rcoles en el Balneario Municipal la inauguraci√≥n oficial de la temporada de verano 2024-2025. A\n",
      "\n",
      "üìù Listado: Rosario Romero destac√≥ las obras realizadas en Paran√° tras cumplirse el primer a√±o de gesti√≥n\n",
      "üëâ Contenido truncado: Al cumplirse un a√±o del comienzo de su gesti√≥n, la intendenta de Paran√°, Rosario Romero, realiz√≥ un balance de los distintos ejes de su plan de gobierno. En este marco, habl√≥ del desaf√≠o que signific√≥\n",
      "\n",
      "üìù Listado: La intendenta Romero destac√≥ a la colectividad piemontesa por honrar la cultura del trabajo\n",
      "üëâ Contenido truncado: ‚ÄúAdmiro la tarea de todas las colectividades y en especial la piemontesa, me toca mucho porque tengo ascendencia por parte de mi madre. Su padre era de Torino, es decir, en pleno Piemonte. Siempre rec\n",
      "\n",
      "üìù Listado: ATE present√≥ un proyecto de Convenio Colectivo de Trabajo a Rosario Romero\n",
      "üëâ Contenido truncado: Oscar Muntes, secretario general y dirigente de la Asociaci√≥n Trabajadores del Estado se reuni√≥ con la intendenta Rosario Romero y parte de su gabinete de gobierno en el que el gremio present√≥ una pro\n",
      "\n",
      "üìù Listado: Rosario Romero y Alicia Aluani firmaron un convenio\n",
      "üëâ Contenido truncado: ‚ÄúEl prop√≥sito de la Vicegobernadora me parece excelente, en el sentido de que en todas las oficinas haya gente que maneje la lengua de se√±as. Este convenio va a ser una de tantas acciones que vamos a \n",
      "üõë Fin del scroll.\n",
      "\n",
      "‚úÖ Guardado en: C:/Users/Lenovo/Documents/github/seguimiento-de-noticias/data/pruebas/apfdigital_debug.xlsx - Total: 19 noticias.\n"
     ]
    }
   ],
   "source": [
    "# APF DIGITAL\n",
    "\n",
    "import os, re, time\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# === CONFIG ===\n",
    "PALABRAS_CLAVE = [\"lauritto\", \"romero\"]\n",
    "BASE_URL = \"https://www.apfdigital.com.ar\"\n",
    "BUSQUEDA_URL = BASE_URL + \"/buscador/?s={}\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "FECHA_CORTE_DT = datetime.today() - timedelta(days=180)\n",
    "OUT_PATH = r\"C:/Users/Lenovo/Documents/github/seguimiento-de-noticias/data/pruebas/apfdigital_debug.xlsx\"\n",
    "\n",
    "# === DRIVER ===\n",
    "def setup_driver():\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "\n",
    "# === PARSE FECHA ===\n",
    "def parse_fecha_apf(txt):\n",
    "    meses = {\n",
    "        \"enero\": 1, \"febrero\": 2, \"marzo\": 3, \"abril\": 4, \"mayo\": 5, \"junio\": 6,\n",
    "        \"julio\": 7, \"agosto\": 8, \"septiembre\": 9, \"octubre\": 10, \"noviembre\": 11, \"diciembre\": 12\n",
    "    }\n",
    "    m = re.search(r'(\\d{1,2})\\s+de\\s+([a-z√°√©√≠√≥√∫]+)\\s+de\\s+(\\d{4})', txt.lower())\n",
    "    if not m: return None\n",
    "    d, mes_str, y = int(m[1]), m[2], int(m[3])\n",
    "    mes = meses.get(mes_str.strip())\n",
    "    if not mes: return None\n",
    "    try:\n",
    "        return datetime(y, mes, d)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# === SCRAPEAR DETALLE ===\n",
    "def scrap_detalle(url, fallback_title):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        titulo = soup.select_one(\"h1.titulo-nota\")\n",
    "        titulo = titulo.get_text(strip=True) if titulo else fallback_title\n",
    "\n",
    "        copete = soup.select_one(\"h2.bajada\")\n",
    "        copete = copete.get_text(strip=True) if copete else \"\"\n",
    "\n",
    "        fecha = None\n",
    "        fecha_data = soup.select_one(\"[data-fecha]\")\n",
    "        if fecha_data and fecha_data.get(\"data-fecha\"):\n",
    "            try:\n",
    "                fecha = datetime.strptime(fecha_data[\"data-fecha\"], \"%Y/%m/%d %H:%M:%S\")\n",
    "            except:\n",
    "                pass\n",
    "        if not fecha:\n",
    "            fecha_txt = soup.select_one(\"div[class*='fecha'], span[class*='fecha'], time\")\n",
    "            if fecha_txt:\n",
    "                fecha = parse_fecha_apf(fecha_txt.get_text(strip=True))\n",
    "\n",
    "        cuerpo = (\n",
    "            soup.select_one(\"div#cuerpo-nota\")\n",
    "            or soup.select_one(\"div.texto\")\n",
    "            or soup.select_one(\"div.noticia-contenido\")\n",
    "        )\n",
    "        texto = \" \".join(p.get_text(strip=True) for p in cuerpo.find_all([\"p\", \"h3\"])) if cuerpo else \"\"\n",
    "        contenido = (copete + \". \" + texto).strip() if copete else texto.strip()\n",
    "\n",
    "        if not contenido:\n",
    "            print(\"‚ö†Ô∏è Contenido vac√≠o:\", url)\n",
    "        else:\n",
    "            print(\"üëâ Contenido truncado:\", contenido[:200])\n",
    "\n",
    "        return {\n",
    "            \"fecha\": fecha.strftime(\"%Y-%m-%d\") if fecha else None,\n",
    "            \"titulo\": titulo,\n",
    "            \"copete\": copete,\n",
    "            \"contenido\": contenido,\n",
    "            \"enlace\": url\n",
    "        }, fecha\n",
    "    except Exception as e:\n",
    "        print(f\"Error detalle {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# === BUSCAR Y EXTRAER ===\n",
    "def buscar_y_extraer(palabra):\n",
    "    driver = setup_driver()\n",
    "    encontrados = []\n",
    "    try:\n",
    "        url = BUSQUEDA_URL.format(quote_plus(palabra))\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        print(f\"\\nüîç Buscando: {palabra}\")\n",
    "\n",
    "        while True:\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            cards = soup.select(\"article.listado-noticias-relacionadas\")\n",
    "            if not cards:\n",
    "                print(\"No se encontraron resultados.\")\n",
    "                break\n",
    "\n",
    "            for card in cards:\n",
    "                h2 = card.select_one(\"h2.text-noticia-simple-titulo\")\n",
    "                a = card.select_one(\"a[href]\")\n",
    "                if not h2 or not a:\n",
    "                    continue\n",
    "                titulo = h2.get_text(strip=True)\n",
    "                href = BASE_URL + a[\"href\"]\n",
    "\n",
    "                print(f\"\\nüìù Listado: {titulo}\")\n",
    "                row, fecha = scrap_detalle(href, titulo)\n",
    "\n",
    "                if not row or not fecha or fecha < FECHA_CORTE_DT:\n",
    "                    continue\n",
    "\n",
    "                row[\"palabra_clave\"] = palabra\n",
    "                encontrados.append(row)\n",
    "\n",
    "            try:\n",
    "                btn = driver.find_element(By.XPATH, \"//a[contains(translate(.,'VER M√ÅS','ver m√°s'),'ver m√°s')]\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", btn)\n",
    "                time.sleep(0.5)\n",
    "                btn.click()\n",
    "                time.sleep(2)\n",
    "            except NoSuchElementException:\n",
    "                print(\"üõë Fin del scroll.\")\n",
    "                break\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return encontrados\n",
    "\n",
    "# === EJECUCI√ìN PRINCIPAL ===\n",
    "if __name__ == \"__main__\":\n",
    "    todos = []\n",
    "    for palabra in PALABRAS_CLAVE:\n",
    "        notas = buscar_y_extraer(palabra)\n",
    "        todos.extend(notas)\n",
    "\n",
    "    if todos:\n",
    "        df = pd.DataFrame(todos).drop_duplicates(subset=[\"enlace\"]).reset_index(drop=True)\n",
    "        os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "        df.to_excel(OUT_PATH, index=False)\n",
    "        print(f\"\\n‚úÖ Guardado en: {OUT_PATH} - Total: {len(df)} noticias.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No se encontraron noticias.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6458a435",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    130\u001b[39m todos = []\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m palabra \u001b[38;5;129;01min\u001b[39;00m PALABRAS_CLAVE:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     notas = \u001b[43mbuscar_y_extraer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpalabra\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     todos.extend(notas)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m todos:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mbuscar_y_extraer\u001b[39m\u001b[34m(palabra)\u001b[39m\n\u001b[32m    106\u001b[39m url_abs = urljoin(BASE_URL, href)\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìù Listado: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitulo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m row = \u001b[43mscrap_detalle\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_abs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitulo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m row: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    112\u001b[39m row[\u001b[33m\"\u001b[39m\u001b[33mpalabra_clave\u001b[39m\u001b[33m\"\u001b[39m] = palabra\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mscrap_detalle\u001b[39m\u001b[34m(url, fallback_title)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscrap_detalle\u001b[39m(url, fallback_title):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m         r.raise_for_status()\n\u001b[32m     36\u001b[39m         soup = BeautifulSoup(r.text, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\urllib3\\response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\urllib3\\response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\urllib3\\response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ELONCE\n",
    "\n",
    "import os, re, time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, quote_plus\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# === CONFIG ===\n",
    "PALABRAS_CLAVE = [\"lauritto\", \"romero\"]\n",
    "BASE_URL = \"https://www.elonce.com\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "OUT_PATH = r\"C:/Users/Lenovo/Documents/github/seguimiento-de-noticias/data/pruebas/elonce_debug.xlsx\"\n",
    "\n",
    "# === SETUP DRIVER ===\n",
    "def setup_driver():\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "\n",
    "# === PARSE DETALLE ===\n",
    "def scrap_detalle(url, fallback_title):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        titulo = soup.select_one(\"h1.titulo-nota\") or soup.select_one(\"h1\")\n",
    "        titulo = titulo.get_text(strip=True) if titulo else fallback_title\n",
    "\n",
    "        copete_tag = soup.select_one(\"h2.bajada\")\n",
    "        copete = copete_tag.get_text(strip=True) if copete_tag else \"\"\n",
    "\n",
    "        fecha_tag = soup.select_one(\"span.fecha-nota\")\n",
    "        fecha_str = fecha_tag.get_text(strip=True) if fecha_tag else \"\"\n",
    "        fecha_dt = None\n",
    "        m = re.search(r\"(\\d{1,2})\\s+de\\s+([a-z√°√©√≠√≥√∫]+)\\s+de\\s+(\\d{4})\", fecha_str.lower())\n",
    "        if m:\n",
    "            meses = {\n",
    "                \"enero\":1,\"febrero\":2,\"marzo\":3,\"abril\":4,\"mayo\":5,\"junio\":6,\n",
    "                \"julio\":7,\"agosto\":8,\"septiembre\":9,\"octubre\":10,\"noviembre\":11,\"diciembre\":12\n",
    "            }\n",
    "            d, mes_str, y = int(m[1]), m[2], int(m[3])\n",
    "            mes = meses.get(mes_str.strip())\n",
    "            if mes:\n",
    "                try: fecha_dt = datetime(y, mes, d)\n",
    "                except: pass\n",
    "\n",
    "        cuerpo_div = (\n",
    "            soup.select_one(\"div.texto\") or\n",
    "            soup.select_one(\"div.noticia-contenido\") or\n",
    "            soup.select_one(\"div.cuerpo-nota\")\n",
    "        )\n",
    "        texto = \" \".join(p.get_text(strip=True) for p in cuerpo_div.find_all([\"p\", \"h3\"])) if cuerpo_div else \"\"\n",
    "        contenido = (copete + \". \" + texto).strip() if copete else texto.strip()\n",
    "\n",
    "        if not contenido:\n",
    "            print(\"‚ö†Ô∏è Contenido vac√≠o:\", url)\n",
    "        else:\n",
    "            print(\"üëâ Contenido truncado:\", contenido[:200])\n",
    "\n",
    "        return {\n",
    "            \"fecha\": fecha_dt.strftime(\"%Y-%m-%d\") if fecha_dt else None,\n",
    "            \"titulo\": titulo,\n",
    "            \"copete\": copete,\n",
    "            \"contenido\": contenido,\n",
    "            \"enlace\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error detalle {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# === BUSCAR Y EXTRAER ===\n",
    "def buscar_y_extraer(palabra):\n",
    "    encontrados = []\n",
    "    drv = setup_driver()\n",
    "    try:\n",
    "        url = f\"{BASE_URL}/buscador/?q={quote_plus(palabra)}&enviar=Buscar&ord=desc\"\n",
    "        drv.get(url)\n",
    "        time.sleep(2)\n",
    "        print(f\"\\nüîç Buscando: {palabra}\")\n",
    "\n",
    "        while True:\n",
    "            soup = BeautifulSoup(drv.page_source, \"html.parser\")\n",
    "            articulos = soup.select(\"article.en-bandera--listado\")\n",
    "            if not articulos:\n",
    "                print(\"No se encontraron art√≠culos.\")\n",
    "                break\n",
    "\n",
    "            for art in articulos:\n",
    "                enlace_tag = art.select_one(\"a.en-bandera__ancla-title\")\n",
    "                if not enlace_tag:\n",
    "                    continue\n",
    "                href = enlace_tag.get(\"href\")\n",
    "                titulo = enlace_tag.get_text(strip=True)\n",
    "                url_abs = urljoin(BASE_URL, href)\n",
    "\n",
    "                print(f\"\\nüìù Listado: {titulo}\")\n",
    "                row = scrap_detalle(url_abs, titulo)\n",
    "                if not row: continue\n",
    "\n",
    "                row[\"palabra_clave\"] = palabra\n",
    "                encontrados.append(row)\n",
    "\n",
    "            try:\n",
    "                btn = drv.find_element(By.CLASS_NAME, \"ver-mas\")\n",
    "                drv.execute_script(\"arguments[0].scrollIntoView({behavior: 'auto', block: 'center'});\", btn)\n",
    "                time.sleep(1)\n",
    "                drv.execute_script(\"arguments[0].click();\", btn)\n",
    "                time.sleep(2)\n",
    "            except NoSuchElementException:\n",
    "                print(\"üõë Fin scroll.\")\n",
    "                break\n",
    "    finally:\n",
    "        drv.quit()\n",
    "    return encontrados\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    todos = []\n",
    "    for palabra in PALABRAS_CLAVE:\n",
    "        notas = buscar_y_extraer(palabra)\n",
    "        todos.extend(notas)\n",
    "\n",
    "    if todos:\n",
    "        df = pd.DataFrame(todos).drop_duplicates(subset=[\"enlace\"]).reset_index(drop=True)\n",
    "        os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "        df.to_excel(OUT_PATH, index=False)\n",
    "        print(f\"\\n‚úÖ Guardado en: {OUT_PATH} - Total: {len(df)} noticias.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No se encontraron noticias.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313308dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Buscando: lauritto - P√°gina 1: https://www.unoentrerios.com.ar/contenidos/resultado.html?search=lauritto&gsc.page=0\n",
      "‚ö†Ô∏è No se encontraron resultados.\n",
      "üîç Buscando: romero - P√°gina 1: https://www.unoentrerios.com.ar/contenidos/resultado.html?search=romero&gsc.page=0\n",
      "‚ö†Ô∏è No se encontraron resultados.\n",
      "\n",
      "‚ö†Ô∏è No se encontraron noticias relevantes.\n"
     ]
    }
   ],
   "source": [
    "# Scraper UNO Digital - Prueba con buscador interno\n",
    "# ======================================\n",
    "# Busca por palabra clave en:\n",
    "# https://www.unoentrerios.com.ar/contenidos/resultado.html?search=PALABRA\n",
    "# No usa Selenium. P√°ginas 1 a 3.\n",
    "# ======================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, quote_plus\n",
    "from datetime import datetime\n",
    "\n",
    "# === CONFIG ===\n",
    "PALABRAS_CLAVE = [\"lauritto\", \"romero\"]\n",
    "BASE_URL = \"https://www.unoentrerios.com.ar\"\n",
    "OUT_PATH = r\"C:/Users/Lenovo/Documents/github/seguimiento-de-noticias/data/pruebas/unodigital_test.xlsx\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# === PARSEADOR DE NOTA ===\n",
    "def scrap_detalle(url, fallback_title):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        titulo = soup.find(\"h1\")\n",
    "        titulo = titulo.get_text(strip=True) if titulo else fallback_title\n",
    "\n",
    "        copete_tag = soup.find(\"h2\")\n",
    "        copete = copete_tag.get_text(strip=True) if copete_tag else \"\"\n",
    "\n",
    "        contenido_div = soup.find(\"div\", class_=\"article-body\") or soup.find(\"div\", class_=\"nota-contenido\")\n",
    "        contenido = \" \".join(p.get_text(strip=True) for p in contenido_div.find_all(\"p\")) if contenido_div else \"\"\n",
    "\n",
    "        texto = (copete + \". \" + contenido).strip() if copete else contenido\n",
    "\n",
    "        fecha_tag = soup.select_one(\"span.fecha, .article-date, .nota-fecha\")\n",
    "        fecha_txt = fecha_tag.get_text(strip=True) if fecha_tag else \"\"\n",
    "        fecha_dt = None\n",
    "        m = re.search(r'(\\d{1,2})\\s+de\\s+([a-z√°√©√≠√≥√∫]+)\\s+de\\s+(\\d{4})', fecha_txt.lower())\n",
    "        if m:\n",
    "            meses = {\n",
    "                \"enero\":1,\"febrero\":2,\"marzo\":3,\"abril\":4,\"mayo\":5,\"junio\":6,\n",
    "                \"julio\":7,\"agosto\":8,\"septiembre\":9,\"octubre\":10,\"noviembre\":11,\"diciembre\":12\n",
    "            }\n",
    "            d, mes_str, y = int(m[1]), m[2], int(m[3])\n",
    "            mes = meses.get(mes_str.strip())\n",
    "            if mes:\n",
    "                try: fecha_dt = datetime(y, mes, d)\n",
    "                except: pass\n",
    "\n",
    "        return {\n",
    "            \"titulo\": titulo,\n",
    "            \"copete\": copete,\n",
    "            \"contenido\": texto,\n",
    "            \"fecha\": fecha_dt.strftime(\"%Y-%m-%d\") if fecha_dt else None,\n",
    "            \"enlace\": url,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error detalle {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# === FUNCION PRINCIPAL ===\n",
    "def buscar_palabra(palabra):\n",
    "    resultados = []\n",
    "    for pagina in range(3):\n",
    "        url = f\"{BASE_URL}/contenidos/resultado.html?search={quote_plus(palabra)}&gsc.page={pagina}\"\n",
    "        print(f\"üîç Buscando: {palabra} - P√°gina {pagina+1}: {url}\")\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            enlaces = soup.select(\"div.gsc-webResult div.gs-title a.gs-title\")\n",
    "            if not enlaces:\n",
    "                print(\"‚ö†Ô∏è No se encontraron resultados.\")\n",
    "                break\n",
    "\n",
    "            for a in enlaces:\n",
    "                href = a.get(\"href\")\n",
    "                if href and BASE_URL in href:\n",
    "                    titulo = a.get_text(strip=True)\n",
    "                    print(\"üìù\", titulo[:90])\n",
    "                    nota = scrap_detalle(href, titulo)\n",
    "                    if nota:\n",
    "                        nota[\"palabra_clave\"] = palabra\n",
    "                        resultados.append(nota)\n",
    "\n",
    "            time.sleep(1.2)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error en b√∫squeda: {e}\")\n",
    "            continue\n",
    "    return resultados\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    todas = []\n",
    "    for palabra in PALABRAS_CLAVE:\n",
    "        todas.extend(buscar_palabra(palabra))\n",
    "\n",
    "    if todas:\n",
    "        df = pd.DataFrame(todas).drop_duplicates(subset=\"enlace\")\n",
    "        os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "        df.to_excel(OUT_PATH, index=False)\n",
    "        print(f\"\\n‚úÖ Guardado: {len(df)} noticias en {OUT_PATH}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No se encontraron noticias relevantes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffc4aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
