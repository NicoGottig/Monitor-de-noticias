{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cb7468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto fecha: 26 de Junio de 2025\n",
      "Fecha parseada: 2025-06-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://www.elonce.com/politica/juntos-por-entre-rios-apoyan-decision-de-frigerio-de-congelar-sueldos.htm\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "# Buscar la fecha con el selector correcto\n",
    "fecha_tag = soup.select_one(\"span.fecha-nota\")\n",
    "if fecha_tag:\n",
    "    fecha_txt = fecha_tag.get_text(strip=True)\n",
    "    print(\"Texto fecha:\", fecha_txt)\n",
    "    # Ahora parseamos tipo \"26 de Junio de 2025\"\n",
    "    meses = {'enero': 1, 'febrero': 2, 'marzo': 3, 'abril': 4, 'mayo': 5, 'junio': 6,\n",
    "             'julio': 7, 'agosto': 8, 'septiembre': 9, 'octubre': 10, 'noviembre': 11, 'diciembre': 12}\n",
    "    m = re.search(r\"(\\d{1,2})\\s+de\\s+([a-záéíóú]+)\\s+de\\s+(\\d{4})\", fecha_txt.lower())\n",
    "    if m:\n",
    "        d, mes, y = int(m.group(1)), m.group(2), int(m.group(3))\n",
    "        fecha = datetime(y, meses[mes], d)\n",
    "        print(\"Fecha parseada:\", fecha)\n",
    "    else:\n",
    "        print(\"No se pudo parsear la fecha.\")\n",
    "else:\n",
    "    print(\"No se encontró span.fecha-nota\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c337d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52edad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - ====== WebDriver manager ======\n",
      "INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha de corte fija: 2025-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Get LATEST chromedriver version for google-chrome\n",
      "INFO - Get LATEST chromedriver version for google-chrome\n",
      "INFO - Get LATEST chromedriver version for google-chrome\n",
      "INFO - Get LATEST chromedriver version for google-chrome\n",
      "INFO - Driver [C:\\Users\\Lenovo\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.168\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "INFO - Driver [C:\\Users\\Lenovo\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.168\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "INFO - ===== Keyword: intendenta romero =====\n",
      "INFO - ===== Keyword: intendenta romero =====\n",
      "INFO - Página 1 - Links acumulados: 20\n",
      "INFO - Página 1 - Links acumulados: 20\n",
      "INFO - Página 2 - Links acumulados: 40\n",
      "INFO - Página 2 - Links acumulados: 40\n",
      "INFO - Página 3 - Links acumulados: 60\n",
      "INFO - Página 3 - Links acumulados: 60\n",
      "INFO - Página 4 - Links acumulados: 80\n",
      "INFO - Página 4 - Links acumulados: 80\n",
      "INFO - Página 5 - Links acumulados: 100\n",
      "INFO - Página 5 - Links acumulados: 100\n",
      "INFO - Página 6 - Links acumulados: 120\n",
      "INFO - Página 6 - Links acumulados: 120\n",
      "INFO - Página 7 - Links acumulados: 140\n",
      "INFO - Página 7 - Links acumulados: 140\n",
      "INFO - Página 8 - Links acumulados: 160\n",
      "INFO - Página 8 - Links acumulados: 160\n",
      "INFO - Página 9 - Links acumulados: 180\n",
      "INFO - Página 9 - Links acumulados: 180\n",
      "INFO - Página 10 - Links acumulados: 200\n",
      "INFO - Página 10 - Links acumulados: 200\n",
      "INFO - Página 11 - Links acumulados: 220\n",
      "INFO - Página 11 - Links acumulados: 220\n",
      "INFO - Página 12 - Links acumulados: 240\n",
      "INFO - Página 12 - Links acumulados: 240\n",
      "INFO - Página 13 - Links acumulados: 260\n",
      "INFO - Página 13 - Links acumulados: 260\n",
      "INFO - Página 14 - Links acumulados: 280\n",
      "INFO - Página 14 - Links acumulados: 280\n",
      "INFO - Página 15 - Links acumulados: 300\n",
      "INFO - Página 15 - Links acumulados: 300\n",
      "INFO - Página 16 - Links acumulados: 320\n",
      "INFO - Página 16 - Links acumulados: 320\n",
      "INFO - Página 17 - Links acumulados: 340\n",
      "INFO - Página 17 - Links acumulados: 340\n",
      "INFO - Página 18 - Links acumulados: 344\n",
      "INFO - Página 18 - Links acumulados: 344\n",
      "INFO - Fecha menor al corte detectada (2024-12-18). Deteniendo scroll.\n",
      "INFO - Fecha menor al corte detectada (2024-12-18). Deteniendo scroll.\n",
      "INFO - Voy a scrapear 344 notas\n",
      "INFO - Voy a scrapear 344 notas\n",
      "INFO - Notas procesadas: 20/344\n",
      "INFO - Notas procesadas: 20/344\n",
      "INFO - Notas procesadas: 40/344\n",
      "INFO - Notas procesadas: 40/344\n",
      "INFO - Notas procesadas: 60/344\n",
      "INFO - Notas procesadas: 60/344\n",
      "INFO - Notas procesadas: 80/344\n",
      "INFO - Notas procesadas: 80/344\n",
      "INFO - Notas procesadas: 100/344\n",
      "INFO - Notas procesadas: 100/344\n",
      "INFO - Notas procesadas: 120/344\n",
      "INFO - Notas procesadas: 120/344\n",
      "INFO - Notas procesadas: 140/344\n",
      "INFO - Notas procesadas: 140/344\n",
      "INFO - Notas procesadas: 160/344\n",
      "INFO - Notas procesadas: 160/344\n",
      "INFO - Notas procesadas: 180/344\n",
      "INFO - Notas procesadas: 180/344\n",
      "INFO - Notas procesadas: 200/344\n",
      "INFO - Notas procesadas: 200/344\n",
      "INFO - Notas procesadas: 220/344\n",
      "INFO - Notas procesadas: 220/344\n",
      "INFO - Notas procesadas: 240/344\n",
      "INFO - Notas procesadas: 240/344\n",
      "INFO - Notas procesadas: 260/344\n",
      "INFO - Notas procesadas: 260/344\n",
      "INFO - Notas procesadas: 280/344\n",
      "INFO - Notas procesadas: 280/344\n",
      "INFO - Notas procesadas: 300/344\n",
      "INFO - Notas procesadas: 300/344\n",
      "INFO - Notas procesadas: 320/344\n",
      "INFO - Notas procesadas: 320/344\n",
      "INFO - Notas procesadas: 340/344\n",
      "INFO - Notas procesadas: 340/344\n",
      "INFO - Notas procesadas: 344/344\n",
      "INFO - Notas procesadas: 344/344\n",
      "INFO - ===== Keyword: rosario romero =====\n",
      "INFO - ===== Keyword: rosario romero =====\n",
      "INFO - Página 1 - Links acumulados: 20\n",
      "INFO - Página 1 - Links acumulados: 20\n",
      "INFO - Página 2 - Links acumulados: 40\n",
      "INFO - Página 2 - Links acumulados: 40\n",
      "INFO - Página 3 - Links acumulados: 60\n",
      "INFO - Página 3 - Links acumulados: 60\n",
      "INFO - Página 4 - Links acumulados: 60\n",
      "INFO - Página 4 - Links acumulados: 60\n",
      "INFO - Página 5 - Links acumulados: 80\n",
      "INFO - Página 5 - Links acumulados: 80\n",
      "INFO - Página 6 - Links acumulados: 100\n",
      "INFO - Página 6 - Links acumulados: 100\n",
      "INFO - Página 7 - Links acumulados: 120\n",
      "INFO - Página 7 - Links acumulados: 120\n",
      "INFO - Página 8 - Links acumulados: 140\n",
      "INFO - Página 8 - Links acumulados: 140\n",
      "INFO - Página 9 - Links acumulados: 160\n",
      "INFO - Página 9 - Links acumulados: 160\n",
      "INFO - Página 10 - Links acumulados: 180\n",
      "INFO - Página 10 - Links acumulados: 180\n",
      "INFO - Página 11 - Links acumulados: 200\n",
      "INFO - Página 11 - Links acumulados: 200\n",
      "INFO - Página 12 - Links acumulados: 220\n",
      "INFO - Página 12 - Links acumulados: 220\n",
      "INFO - Página 13 - Links acumulados: 240\n",
      "INFO - Página 13 - Links acumulados: 240\n",
      "INFO - Página 14 - Links acumulados: 260\n",
      "INFO - Página 14 - Links acumulados: 260\n",
      "INFO - Página 15 - Links acumulados: 280\n",
      "INFO - Página 15 - Links acumulados: 280\n",
      "INFO - Página 16 - Links acumulados: 300\n",
      "INFO - Página 16 - Links acumulados: 300\n",
      "INFO - Página 17 - Links acumulados: 320\n",
      "INFO - Página 17 - Links acumulados: 320\n",
      "INFO - Página 18 - Links acumulados: 340\n",
      "INFO - Página 18 - Links acumulados: 340\n",
      "INFO - Página 19 - Links acumulados: 360\n",
      "INFO - Página 19 - Links acumulados: 360\n",
      "INFO - Página 20 - Links acumulados: 380\n",
      "INFO - Página 20 - Links acumulados: 380\n",
      "INFO - Página 21 - Links acumulados: 400\n",
      "INFO - Página 21 - Links acumulados: 400\n",
      "INFO - Página 22 - Links acumulados: 416\n",
      "INFO - Página 22 - Links acumulados: 416\n",
      "INFO - Fecha menor al corte detectada (2024-12-28). Deteniendo scroll.\n",
      "INFO - Fecha menor al corte detectada (2024-12-28). Deteniendo scroll.\n",
      "INFO - Voy a scrapear 416 notas\n",
      "INFO - Voy a scrapear 416 notas\n",
      "INFO - Notas procesadas: 20/416\n",
      "INFO - Notas procesadas: 20/416\n",
      "INFO - Notas procesadas: 40/416\n",
      "INFO - Notas procesadas: 40/416\n",
      "INFO - Notas procesadas: 60/416\n",
      "INFO - Notas procesadas: 60/416\n",
      "INFO - Notas procesadas: 80/416\n",
      "INFO - Notas procesadas: 80/416\n",
      "INFO - Notas procesadas: 100/416\n",
      "INFO - Notas procesadas: 100/416\n",
      "INFO - Notas procesadas: 120/416\n",
      "INFO - Notas procesadas: 120/416\n",
      "INFO - Notas procesadas: 140/416\n",
      "INFO - Notas procesadas: 140/416\n",
      "INFO - Notas procesadas: 160/416\n",
      "INFO - Notas procesadas: 160/416\n",
      "INFO - Notas procesadas: 180/416\n",
      "INFO - Notas procesadas: 180/416\n",
      "INFO - Notas procesadas: 200/416\n",
      "INFO - Notas procesadas: 200/416\n",
      "INFO - Notas procesadas: 220/416\n",
      "INFO - Notas procesadas: 220/416\n",
      "INFO - Notas procesadas: 240/416\n",
      "INFO - Notas procesadas: 240/416\n",
      "INFO - Notas procesadas: 260/416\n",
      "INFO - Notas procesadas: 260/416\n",
      "INFO - Notas procesadas: 280/416\n",
      "INFO - Notas procesadas: 280/416\n",
      "INFO - Notas procesadas: 300/416\n",
      "INFO - Notas procesadas: 300/416\n",
      "INFO - Notas procesadas: 320/416\n",
      "INFO - Notas procesadas: 320/416\n",
      "INFO - Notas procesadas: 340/416\n",
      "INFO - Notas procesadas: 340/416\n",
      "INFO - Notas procesadas: 360/416\n",
      "INFO - Notas procesadas: 360/416\n",
      "INFO - Notas procesadas: 380/416\n",
      "INFO - Notas procesadas: 380/416\n",
      "INFO - Notas procesadas: 400/416\n",
      "INFO - Notas procesadas: 400/416\n",
      "INFO - Notas procesadas: 416/416\n",
      "INFO - Notas procesadas: 416/416\n",
      "INFO - ===== Keyword: frigerio =====\n",
      "INFO - ===== Keyword: frigerio =====\n",
      "INFO - Página 1 - Links acumulados: 20\n",
      "INFO - Página 1 - Links acumulados: 20\n",
      "INFO - Página 2 - Links acumulados: 20\n",
      "INFO - Página 2 - Links acumulados: 20\n",
      "INFO - Página 3 - Links acumulados: 40\n",
      "INFO - Página 3 - Links acumulados: 40\n",
      "INFO - Página 4 - Links acumulados: 60\n",
      "INFO - Página 4 - Links acumulados: 60\n",
      "INFO - Página 5 - Links acumulados: 80\n",
      "INFO - Página 5 - Links acumulados: 80\n",
      "INFO - Página 6 - Links acumulados: 100\n",
      "INFO - Página 6 - Links acumulados: 100\n",
      "INFO - Página 7 - Links acumulados: 120\n",
      "INFO - Página 7 - Links acumulados: 120\n",
      "INFO - Página 8 - Links acumulados: 140\n",
      "INFO - Página 8 - Links acumulados: 140\n",
      "INFO - Página 9 - Links acumulados: 160\n",
      "INFO - Página 9 - Links acumulados: 160\n",
      "INFO - Página 10 - Links acumulados: 180\n",
      "INFO - Página 10 - Links acumulados: 180\n",
      "INFO - Página 11 - Links acumulados: 200\n",
      "INFO - Página 11 - Links acumulados: 200\n",
      "INFO - Página 12 - Links acumulados: 220\n",
      "INFO - Página 12 - Links acumulados: 220\n",
      "INFO - Página 13 - Links acumulados: 240\n",
      "INFO - Página 13 - Links acumulados: 240\n",
      "INFO - Página 14 - Links acumulados: 260\n",
      "INFO - Página 14 - Links acumulados: 260\n",
      "INFO - Página 15 - Links acumulados: 280\n",
      "INFO - Página 15 - Links acumulados: 280\n",
      "INFO - Página 16 - Links acumulados: 300\n",
      "INFO - Página 16 - Links acumulados: 300\n",
      "INFO - Página 17 - Links acumulados: 320\n",
      "INFO - Página 17 - Links acumulados: 320\n",
      "INFO - Página 18 - Links acumulados: 320\n",
      "INFO - Página 18 - Links acumulados: 320\n",
      "INFO - Página 19 - Links acumulados: 360\n",
      "INFO - Página 19 - Links acumulados: 360\n",
      "INFO - Página 20 - Links acumulados: 380\n",
      "INFO - Página 20 - Links acumulados: 380\n",
      "INFO - Página 21 - Links acumulados: 400\n",
      "INFO - Página 21 - Links acumulados: 400\n",
      "INFO - Página 22 - Links acumulados: 420\n",
      "INFO - Página 22 - Links acumulados: 420\n",
      "INFO - Página 23 - Links acumulados: 440\n",
      "INFO - Página 23 - Links acumulados: 440\n",
      "INFO - Página 24 - Links acumulados: 460\n",
      "INFO - Página 24 - Links acumulados: 460\n",
      "INFO - Página 25 - Links acumulados: 480\n",
      "INFO - Página 25 - Links acumulados: 480\n",
      "INFO - Página 26 - Links acumulados: 500\n",
      "INFO - Página 26 - Links acumulados: 500\n",
      "INFO - Página 27 - Links acumulados: 520\n",
      "INFO - Página 27 - Links acumulados: 520\n",
      "INFO - Página 28 - Links acumulados: 540\n",
      "INFO - Página 28 - Links acumulados: 540\n",
      "INFO - Página 29 - Links acumulados: 560\n",
      "INFO - Página 29 - Links acumulados: 560\n",
      "INFO - Página 30 - Links acumulados: 580\n",
      "INFO - Página 30 - Links acumulados: 580\n",
      "INFO - Página 31 - Links acumulados: 600\n",
      "INFO - Página 31 - Links acumulados: 600\n",
      "INFO - Página 32 - Links acumulados: 620\n",
      "INFO - Página 32 - Links acumulados: 620\n",
      "INFO - Página 33 - Links acumulados: 640\n",
      "INFO - Página 33 - Links acumulados: 640\n",
      "INFO - Página 34 - Links acumulados: 660\n",
      "INFO - Página 34 - Links acumulados: 660\n",
      "INFO - Página 35 - Links acumulados: 680\n",
      "INFO - Página 35 - Links acumulados: 680\n",
      "INFO - Página 36 - Links acumulados: 700\n",
      "INFO - Página 36 - Links acumulados: 700\n",
      "INFO - Página 37 - Links acumulados: 716\n",
      "INFO - Página 37 - Links acumulados: 716\n",
      "INFO - Fecha menor al corte detectada (2024-12-30). Deteniendo scroll.\n",
      "INFO - Fecha menor al corte detectada (2024-12-30). Deteniendo scroll.\n",
      "INFO - Voy a scrapear 716 notas\n",
      "INFO - Voy a scrapear 716 notas\n",
      "INFO - Notas procesadas: 20/716\n",
      "INFO - Notas procesadas: 20/716\n",
      "INFO - Notas procesadas: 40/716\n",
      "INFO - Notas procesadas: 40/716\n",
      "INFO - Notas procesadas: 60/716\n",
      "INFO - Notas procesadas: 60/716\n",
      "INFO - Notas procesadas: 80/716\n",
      "INFO - Notas procesadas: 80/716\n",
      "INFO - Notas procesadas: 100/716\n",
      "INFO - Notas procesadas: 100/716\n",
      "INFO - Notas procesadas: 120/716\n",
      "INFO - Notas procesadas: 120/716\n",
      "INFO - Notas procesadas: 140/716\n",
      "INFO - Notas procesadas: 140/716\n",
      "INFO - Notas procesadas: 160/716\n",
      "INFO - Notas procesadas: 160/716\n",
      "INFO - Notas procesadas: 180/716\n",
      "INFO - Notas procesadas: 180/716\n",
      "INFO - Notas procesadas: 200/716\n",
      "INFO - Notas procesadas: 200/716\n",
      "INFO - Notas procesadas: 220/716\n",
      "INFO - Notas procesadas: 220/716\n",
      "INFO - Notas procesadas: 240/716\n",
      "INFO - Notas procesadas: 240/716\n",
      "INFO - Notas procesadas: 260/716\n",
      "INFO - Notas procesadas: 260/716\n",
      "INFO - Notas procesadas: 280/716\n",
      "INFO - Notas procesadas: 280/716\n",
      "INFO - Notas procesadas: 300/716\n",
      "INFO - Notas procesadas: 300/716\n",
      "INFO - Notas procesadas: 320/716\n",
      "INFO - Notas procesadas: 320/716\n",
      "INFO - Notas procesadas: 340/716\n",
      "INFO - Notas procesadas: 340/716\n",
      "INFO - Notas procesadas: 360/716\n",
      "INFO - Notas procesadas: 360/716\n",
      "INFO - Notas procesadas: 380/716\n",
      "INFO - Notas procesadas: 380/716\n",
      "INFO - Notas procesadas: 400/716\n",
      "INFO - Notas procesadas: 400/716\n",
      "INFO - Notas procesadas: 420/716\n",
      "INFO - Notas procesadas: 420/716\n",
      "INFO - Notas procesadas: 440/716\n",
      "INFO - Notas procesadas: 440/716\n",
      "INFO - Notas procesadas: 460/716\n",
      "INFO - Notas procesadas: 460/716\n",
      "INFO - Notas procesadas: 480/716\n",
      "INFO - Notas procesadas: 480/716\n",
      "INFO - Notas procesadas: 500/716\n",
      "INFO - Notas procesadas: 500/716\n",
      "INFO - Notas procesadas: 520/716\n",
      "INFO - Notas procesadas: 520/716\n",
      "INFO - Notas procesadas: 540/716\n",
      "INFO - Notas procesadas: 540/716\n",
      "INFO - Notas procesadas: 560/716\n",
      "INFO - Notas procesadas: 560/716\n",
      "INFO - Notas procesadas: 580/716\n",
      "INFO - Notas procesadas: 580/716\n",
      "INFO - Notas procesadas: 600/716\n",
      "INFO - Notas procesadas: 600/716\n",
      "INFO - Notas procesadas: 620/716\n",
      "INFO - Notas procesadas: 620/716\n",
      "INFO - Notas procesadas: 640/716\n",
      "INFO - Notas procesadas: 640/716\n",
      "INFO - Notas procesadas: 660/716\n",
      "INFO - Notas procesadas: 660/716\n",
      "INFO - Notas procesadas: 680/716\n",
      "INFO - Notas procesadas: 680/716\n",
      "INFO - Notas procesadas: 700/716\n",
      "INFO - Notas procesadas: 700/716\n",
      "INFO - Notas procesadas: 716/716\n",
      "INFO - Notas procesadas: 716/716\n",
      "INFO - ===== Keyword: gobernador frigerio =====\n",
      "INFO - ===== Keyword: gobernador frigerio =====\n",
      "INFO - Página 1 - Links acumulados: 20\n",
      "INFO - Página 1 - Links acumulados: 20\n",
      "INFO - Página 2 - Links acumulados: 40\n",
      "INFO - Página 2 - Links acumulados: 40\n",
      "INFO - Página 3 - Links acumulados: 60\n",
      "INFO - Página 3 - Links acumulados: 60\n",
      "INFO - Página 4 - Links acumulados: 80\n",
      "INFO - Página 4 - Links acumulados: 80\n",
      "INFO - Página 5 - Links acumulados: 100\n",
      "INFO - Página 5 - Links acumulados: 100\n",
      "INFO - Página 6 - Links acumulados: 120\n",
      "INFO - Página 6 - Links acumulados: 120\n",
      "INFO - Página 7 - Links acumulados: 120\n",
      "INFO - Página 7 - Links acumulados: 120\n",
      "INFO - Página 8 - Links acumulados: 160\n",
      "INFO - Página 8 - Links acumulados: 160\n",
      "INFO - Página 9 - Links acumulados: 180\n",
      "INFO - Página 9 - Links acumulados: 180\n",
      "INFO - Página 10 - Links acumulados: 200\n",
      "INFO - Página 10 - Links acumulados: 200\n",
      "INFO - Página 11 - Links acumulados: 220\n",
      "INFO - Página 11 - Links acumulados: 220\n",
      "INFO - Página 12 - Links acumulados: 240\n",
      "INFO - Página 12 - Links acumulados: 240\n",
      "INFO - Página 13 - Links acumulados: 260\n",
      "INFO - Página 13 - Links acumulados: 260\n",
      "INFO - Página 14 - Links acumulados: 280\n",
      "INFO - Página 14 - Links acumulados: 280\n",
      "INFO - Página 15 - Links acumulados: 300\n",
      "INFO - Página 15 - Links acumulados: 300\n",
      "INFO - Página 16 - Links acumulados: 320\n",
      "INFO - Página 16 - Links acumulados: 320\n",
      "INFO - Página 17 - Links acumulados: 340\n",
      "INFO - Página 17 - Links acumulados: 340\n",
      "INFO - Página 18 - Links acumulados: 360\n",
      "INFO - Página 18 - Links acumulados: 360\n",
      "INFO - Página 19 - Links acumulados: 380\n",
      "INFO - Página 19 - Links acumulados: 380\n",
      "INFO - Página 20 - Links acumulados: 400\n",
      "INFO - Página 20 - Links acumulados: 400\n",
      "INFO - Página 21 - Links acumulados: 420\n",
      "INFO - Página 21 - Links acumulados: 420\n",
      "INFO - Página 22 - Links acumulados: 440\n",
      "INFO - Página 22 - Links acumulados: 440\n",
      "INFO - Página 23 - Links acumulados: 460\n",
      "INFO - Página 23 - Links acumulados: 460\n",
      "INFO - Página 24 - Links acumulados: 460\n",
      "INFO - Página 24 - Links acumulados: 460\n",
      "INFO - Página 25 - Links acumulados: 500\n",
      "INFO - Página 25 - Links acumulados: 500\n",
      "INFO - Página 26 - Links acumulados: 500\n",
      "INFO - Página 26 - Links acumulados: 500\n",
      "INFO - Página 27 - Links acumulados: 540\n",
      "INFO - Página 27 - Links acumulados: 540\n",
      "INFO - Página 28 - Links acumulados: 560\n",
      "INFO - Página 28 - Links acumulados: 560\n",
      "INFO - Página 29 - Links acumulados: 580\n",
      "INFO - Página 29 - Links acumulados: 580\n",
      "INFO - Página 30 - Links acumulados: 600\n",
      "INFO - Página 30 - Links acumulados: 600\n",
      "INFO - Página 31 - Links acumulados: 610\n",
      "INFO - Página 31 - Links acumulados: 610\n",
      "INFO - Fecha menor al corte detectada (2024-12-27). Deteniendo scroll.\n",
      "INFO - Fecha menor al corte detectada (2024-12-27). Deteniendo scroll.\n",
      "INFO - Voy a scrapear 610 notas\n",
      "INFO - Voy a scrapear 610 notas\n",
      "INFO - Notas procesadas: 20/610\n",
      "INFO - Notas procesadas: 20/610\n",
      "INFO - Notas procesadas: 40/610\n",
      "INFO - Notas procesadas: 40/610\n",
      "INFO - Notas procesadas: 60/610\n",
      "INFO - Notas procesadas: 60/610\n",
      "INFO - Notas procesadas: 80/610\n",
      "INFO - Notas procesadas: 80/610\n",
      "INFO - Notas procesadas: 100/610\n",
      "INFO - Notas procesadas: 100/610\n",
      "INFO - Notas procesadas: 120/610\n",
      "INFO - Notas procesadas: 120/610\n",
      "INFO - Notas procesadas: 140/610\n",
      "INFO - Notas procesadas: 140/610\n",
      "INFO - Notas procesadas: 160/610\n",
      "INFO - Notas procesadas: 160/610\n",
      "INFO - Notas procesadas: 180/610\n",
      "INFO - Notas procesadas: 180/610\n",
      "INFO - Notas procesadas: 200/610\n",
      "INFO - Notas procesadas: 200/610\n",
      "INFO - Notas procesadas: 220/610\n",
      "INFO - Notas procesadas: 220/610\n",
      "INFO - Notas procesadas: 240/610\n",
      "INFO - Notas procesadas: 240/610\n",
      "INFO - Notas procesadas: 260/610\n",
      "INFO - Notas procesadas: 260/610\n",
      "INFO - Notas procesadas: 280/610\n",
      "INFO - Notas procesadas: 280/610\n",
      "INFO - Notas procesadas: 300/610\n",
      "INFO - Notas procesadas: 300/610\n",
      "INFO - Notas procesadas: 320/610\n",
      "INFO - Notas procesadas: 320/610\n",
      "INFO - Notas procesadas: 340/610\n",
      "INFO - Notas procesadas: 340/610\n",
      "INFO - Notas procesadas: 360/610\n",
      "INFO - Notas procesadas: 360/610\n",
      "INFO - Notas procesadas: 380/610\n",
      "INFO - Notas procesadas: 380/610\n",
      "INFO - Notas procesadas: 400/610\n",
      "INFO - Notas procesadas: 400/610\n",
      "INFO - Notas procesadas: 420/610\n",
      "INFO - Notas procesadas: 420/610\n",
      "INFO - Notas procesadas: 440/610\n",
      "INFO - Notas procesadas: 440/610\n",
      "INFO - Notas procesadas: 460/610\n",
      "INFO - Notas procesadas: 460/610\n",
      "INFO - Notas procesadas: 480/610\n",
      "INFO - Notas procesadas: 480/610\n",
      "INFO - Notas procesadas: 500/610\n",
      "INFO - Notas procesadas: 500/610\n",
      "INFO - Notas procesadas: 520/610\n",
      "INFO - Notas procesadas: 520/610\n",
      "INFO - Notas procesadas: 540/610\n",
      "INFO - Notas procesadas: 540/610\n",
      "INFO - Notas procesadas: 560/610\n",
      "INFO - Notas procesadas: 560/610\n",
      "INFO - Notas procesadas: 580/610\n",
      "INFO - Notas procesadas: 580/610\n",
      "INFO - Notas procesadas: 600/610\n",
      "INFO - Notas procesadas: 600/610\n",
      "INFO - Notas procesadas: 610/610\n",
      "INFO - Notas procesadas: 610/610\n",
      "INFO - Archivo nuevo guardado: 1099 filas\n",
      "INFO - Archivo nuevo guardado: 1099 filas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevas filas: 1099\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>medio</th>\n",
       "      <th>fecha</th>\n",
       "      <th>seccion</th>\n",
       "      <th>titulo</th>\n",
       "      <th>url</th>\n",
       "      <th>contenido</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4304f5b2cb9710ef7574504c2c113792</td>\n",
       "      <td>elonce</td>\n",
       "      <td>2025-07-27</td>\n",
       "      <td>paraná</td>\n",
       "      <td>Rosario Romero al nuevo arzobispo: “En Paraná,...</td>\n",
       "      <td>https://www.elonce.com/parana/rosario-romero-a...</td>\n",
       "      <td>La intendenta Rosario Romero entregó la Llave ...</td>\n",
       "      <td>intendenta romero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2adc99be4c344acd3ddc3d01a868fe77</td>\n",
       "      <td>elonce</td>\n",
       "      <td>2025-07-26</td>\n",
       "      <td>paraná</td>\n",
       "      <td>Postales de la asunción de Monseñor Raúl Martí...</td>\n",
       "      <td>https://www.elonce.com/parana/postales-de-la-a...</td>\n",
       "      <td>Este sábado se llevó a cabo la ceremonia de as...</td>\n",
       "      <td>intendenta romero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>faf4d2ec6f5081c8d9a67d8b32f62504</td>\n",
       "      <td>elonce</td>\n",
       "      <td>2025-07-26</td>\n",
       "      <td>paraná</td>\n",
       "      <td>La Iglesia Católica celebró la toma de posesió...</td>\n",
       "      <td>https://www.elonce.com/parana/la-iglesia-catol...</td>\n",
       "      <td>La Iglesia Católica vivió un evento central es...</td>\n",
       "      <td>intendenta romero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a55c4777715975fd6b5ac515f8b6e567</td>\n",
       "      <td>elonce</td>\n",
       "      <td>2025-07-26</td>\n",
       "      <td>paraná</td>\n",
       "      <td>Monseñor Raúl Martín asume como nuevo arzobisp...</td>\n",
       "      <td>https://www.elonce.com/parana/monsenor-raul-ma...</td>\n",
       "      <td>La ceremonia de toma de posesión será este sáb...</td>\n",
       "      <td>intendenta romero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ffdbb966067924862af2111ae991d32b</td>\n",
       "      <td>elonce</td>\n",
       "      <td>2025-07-26</td>\n",
       "      <td>panorama municipal</td>\n",
       "      <td>La Municipalidad de Paraná concesionó el nuevo...</td>\n",
       "      <td>https://www.elonce.com/panorama-municipal/la-m...</td>\n",
       "      <td>El servicio estará a cargo de la Unión Transit...</td>\n",
       "      <td>intendenta romero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id   medio       fecha             seccion  \\\n",
       "0  4304f5b2cb9710ef7574504c2c113792  elonce  2025-07-27              paraná   \n",
       "1  2adc99be4c344acd3ddc3d01a868fe77  elonce  2025-07-26              paraná   \n",
       "2  faf4d2ec6f5081c8d9a67d8b32f62504  elonce  2025-07-26              paraná   \n",
       "3  a55c4777715975fd6b5ac515f8b6e567  elonce  2025-07-26              paraná   \n",
       "4  ffdbb966067924862af2111ae991d32b  elonce  2025-07-26  panorama municipal   \n",
       "\n",
       "                                              titulo  \\\n",
       "0  Rosario Romero al nuevo arzobispo: “En Paraná,...   \n",
       "1  Postales de la asunción de Monseñor Raúl Martí...   \n",
       "2  La Iglesia Católica celebró la toma de posesió...   \n",
       "3  Monseñor Raúl Martín asume como nuevo arzobisp...   \n",
       "4  La Municipalidad de Paraná concesionó el nuevo...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.elonce.com/parana/rosario-romero-a...   \n",
       "1  https://www.elonce.com/parana/postales-de-la-a...   \n",
       "2  https://www.elonce.com/parana/la-iglesia-catol...   \n",
       "3  https://www.elonce.com/parana/monsenor-raul-ma...   \n",
       "4  https://www.elonce.com/panorama-municipal/la-m...   \n",
       "\n",
       "                                           contenido            keyword  \n",
       "0  La intendenta Rosario Romero entregó la Llave ...  intendenta romero  \n",
       "1  Este sábado se llevó a cabo la ceremonia de as...  intendenta romero  \n",
       "2  La Iglesia Católica vivió un evento central es...  intendenta romero  \n",
       "3  La ceremonia de toma de posesión será este sáb...  intendenta romero  \n",
       "4  El servicio estará a cargo de la Unión Transit...  intendenta romero  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================\n",
    "# Scraper FULL Elonce (keywords)\n",
    "# ================================\n",
    "# Selenium: scroll + \"ver más\" para obtener links\n",
    "# Requests + BS4: parseo de cada nota\n",
    "# Corte por fecha, incremental CSV, dedupe por id\n",
    "# ================================\n",
    "\n",
    "import os, re, time, random, logging\n",
    "from datetime import datetime, timedelta\n",
    "from collections import OrderedDict\n",
    "from hashlib import md5\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import sys\n",
    "\n",
    "FECHA_CORTE_STR = \"2025-01-01\"\n",
    "FECHA_CORTE_DT = datetime.strptime(FECHA_CORTE_STR, \"%Y-%m-%d\")\n",
    "print(f\"Fecha de corte fija: {FECHA_CORTE_STR}\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "CANDIDATOS            = [\"intendenta romero\", \"rosario romero\", \"frigerio\", \"gobernador frigerio\"]\n",
    "OUT_PATH              = \"../data/raw/elonce.csv\"\n",
    "LOG_PATH              = f\"logs/elonce_{datetime.now().date()}.log\"\n",
    "HEADLESS              = True\n",
    "MAX_NOTAS_POR_CAND    = None                         # None = sin límite\n",
    "FILTRAR_SECCIONES     = False                        # True = sólo política/economía\n",
    "SECCIONES_OK          = {\"política\", \"economía\"}     # usado si FILTRAR_SECCIONES=True\n",
    "BASE_URL              = \"https://www.elonce.com\"\n",
    "HEADERS               = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "TMP_DIR               = \"tmp\"\n",
    "\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- LOGGING ----------\n",
    "logging.basicConfig(\n",
    "    filename=LOG_PATH,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "console.setFormatter(logging.Formatter(\"%(levelname)s - %(message)s\"))\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# ---------- FECHAS ----------\n",
    "MESES_ES = {\n",
    "    \"enero\": 1, \"febrero\": 2, \"marzo\": 3, \"abril\": 4, \"mayo\": 5, \"junio\": 6,\n",
    "    \"julio\": 7, \"agosto\": 8, \"septiembre\": 9, \"octubre\": 10, \"noviembre\": 11, \"diciembre\": 12\n",
    "}\n",
    "def parse_fecha_es(fecha_str: str):\n",
    "    \"\"\"Acepta formatos tipo '26 de Junio de 2025'\"\"\"\n",
    "    if not fecha_str:\n",
    "        return None\n",
    "    f = fecha_str.lower().strip()\n",
    "    m = re.search(r\"(\\d{1,2})\\s+de\\s+([a-záéíóú]+)\\s+de\\s+(\\d{4})\", f)\n",
    "    if not m:\n",
    "        return None\n",
    "    d, mes_str, y = int(m.group(1)), m.group(2), int(m.group(3))\n",
    "    mes = MESES_ES.get(mes_str.strip(\" .\"))\n",
    "    if not mes:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime(y, mes, d)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def make_hash(value: str) -> str:\n",
    "    return md5(value.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def setup_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    return driver\n",
    "\n",
    "def parse_fecha_data_attr(fecha_raw: str):\n",
    "    try:\n",
    "        return datetime.strptime(fecha_raw, \"%Y/%m/%d %H:%M:%S\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def scroll_and_collect_links(driver, fecha_corte: datetime, max_links=None):\n",
    "    total_links = OrderedDict()\n",
    "    pagina = 1\n",
    "    while True:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        articulos = soup.select(\"article.en-bandera--listado\")\n",
    "        if not articulos:\n",
    "            logging.warning(\"Sin artículos en la página %s\", pagina)\n",
    "\n",
    "        fechas_visibles = []\n",
    "        for art in articulos:\n",
    "            header = art.select_one(\"header.en-bandera__header\")\n",
    "            if not header:\n",
    "                continue\n",
    "            enlace_tag = header.select_one(\"a.en-bandera__ancla-title\")\n",
    "            fecha_tag  = header.select_one(\"span.en-bandera__fecha\")\n",
    "            if not (enlace_tag and fecha_tag):\n",
    "                continue\n",
    "\n",
    "            href = enlace_tag.get(\"href\")\n",
    "            fecha_dt = parse_fecha_data_attr(fecha_tag.get(\"data-fecha\"))\n",
    "            if not fecha_dt:\n",
    "                continue\n",
    "\n",
    "            fechas_visibles.append(fecha_dt)\n",
    "            if fecha_dt >= fecha_corte and href not in total_links:\n",
    "                total_links[href] = fecha_dt\n",
    "                if max_links and len(total_links) >= max_links:\n",
    "                    break\n",
    "\n",
    "        logging.info(\"Página %s - Links acumulados: %s\", pagina, len(total_links))\n",
    "\n",
    "        if fechas_visibles and min(fechas_visibles) < fecha_corte:\n",
    "            logging.info(\"Fecha menor al corte detectada (%s). Deteniendo scroll.\", min(fechas_visibles).date())\n",
    "            break\n",
    "        if max_links and len(total_links) >= max_links:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            boton = driver.find_element(By.CLASS_NAME, \"ver-mas\")\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", boton)\n",
    "            time.sleep(0.4)\n",
    "            driver.execute_script(\"arguments[0].click();\", boton)\n",
    "            time.sleep(random.uniform(1.2, 2.2))\n",
    "            pagina += 1\n",
    "        except NoSuchElementException:\n",
    "            logging.info(\"Fin del scroll (no hay botón 'ver más').\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.info(\"Fin del scroll (error click ver más): %s\", e)\n",
    "            break\n",
    "\n",
    "    return total_links\n",
    "\n",
    "def scrap_articulo_requests(url_abs: str, filtrar_secciones=False):\n",
    "    r = requests.get(url_abs, headers=HEADERS, timeout=25)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Sección\n",
    "    seccion_tag = soup.select_one(\"div.cont-volanta a.etiqueta\")\n",
    "    seccion = seccion_tag.get_text(strip=True).lower() if seccion_tag else None\n",
    "    if filtrar_secciones and seccion not in SECCIONES_OK:\n",
    "        return None, None\n",
    "\n",
    "    # Título / Bajada\n",
    "    h1 = soup.select_one(\"h1.titulo-nota\") or soup.select_one(\"h1\")\n",
    "    titulo = h1.get_text(strip=True) if h1 else \"Sin título\"\n",
    "    bajada_tag = soup.select_one(\"h2.bajada\")\n",
    "    copete = bajada_tag.get_text(strip=True) if bajada_tag else \"\"\n",
    "\n",
    "    # === FECHA: este es el fix principal ===\n",
    "    fecha_dt = None\n",
    "\n",
    "    # 1. Probar <span class=\"fecha-nota\">\n",
    "    fecha_tag = soup.select_one(\"span.fecha-nota\")\n",
    "    if fecha_tag:\n",
    "        txt = fecha_tag.get_text(strip=True)\n",
    "        fecha_dt = parse_fecha_es(txt)\n",
    "\n",
    "    # 2. Si no funcionó, intentar con [data-fecha]\n",
    "    if not fecha_dt:\n",
    "        tag_data = soup.select_one(\"[data-fecha]\")\n",
    "        if tag_data and tag_data.get(\"data-fecha\"):\n",
    "            try:\n",
    "                fecha_dt = datetime.strptime(tag_data[\"data-fecha\"], \"%Y/%m/%d %H:%M:%S\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # 3. Fallback: cualquier visible que diga fecha\n",
    "    if not fecha_dt:\n",
    "        tag_txt = soup.select_one(\"div[class*='fecha'], span[class*='fecha'], time\")\n",
    "        txt = tag_txt.get_text(strip=True) if tag_txt else \"\"\n",
    "        fecha_dt = parse_fecha_es(txt)\n",
    "\n",
    "    # Cuerpo\n",
    "    cuerpo_div = (soup.select_one(\"div.texto\")\n",
    "                  or soup.select_one(\"div.noticia-contenido\")\n",
    "                  or soup.select_one(\"div.cuerpo-nota\"))\n",
    "    if cuerpo_div:\n",
    "        texto = \"\\n\".join(p.get_text(strip=True) for p in cuerpo_div.find_all([\"p\",\"h3\"]) if p.get_text(strip=True))\n",
    "    else:\n",
    "        texto = \"\"\n",
    "\n",
    "    row = {\n",
    "        \"id\": make_hash(url_abs),\n",
    "        \"medio\": \"elonce\",\n",
    "        \"fecha\": fecha_dt.strftime(\"%Y-%m-%d\") if fecha_dt else None,\n",
    "        \"seccion\": seccion,\n",
    "        \"titulo\": titulo,\n",
    "        \"url\": url_abs,\n",
    "        \"contenido\": (copete + \" \" + texto).strip()\n",
    "    }\n",
    "    return row, fecha_dt\n",
    "\n",
    "def save_incremental(df: pd.DataFrame, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    if os.path.exists(path):\n",
    "        prev = pd.read_csv(path)\n",
    "        before = len(prev)\n",
    "        df_all = pd.concat([prev, df], ignore_index=True)\n",
    "        df_all.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "        df_all.to_csv(path, index=False)\n",
    "        logging.info(\"Guardado incremental: %s -> %s filas (+%s nuevas)\",\n",
    "                     before, len(df_all), len(df_all) - before)\n",
    "    else:\n",
    "        df.to_csv(path, index=False)\n",
    "        logging.info(\"Archivo nuevo guardado: %s filas\", len(df))\n",
    "\n",
    "def run_full(candidatos, fecha_corte, out_path,\n",
    "             headless=True, max_notas_por_cand=None, filtrar_secciones=False):\n",
    "    resultados = []\n",
    "    drv_scroll = setup_driver(headless=headless)\n",
    "\n",
    "    try:\n",
    "        for kw in candidatos:\n",
    "            logging.info(\"===== Keyword: %s =====\", kw)\n",
    "            url_busqueda = f\"{BASE_URL}/buscador/?q={kw}&enviar=Buscar&ord=desc\"\n",
    "            drv_scroll.get(url_busqueda)\n",
    "            time.sleep(2)\n",
    "\n",
    "            links = scroll_and_collect_links(drv_scroll, fecha_corte=fecha_corte, max_links=max_notas_por_cand)\n",
    "\n",
    "            total = len(links)\n",
    "            logging.info(\"Voy a scrapear %s notas\", total)\n",
    "\n",
    "            for i, (rel, fdt) in enumerate(links.items(), start=1):\n",
    "                if max_notas_por_cand and i > max_notas_por_cand:\n",
    "                    break\n",
    "                url_abs = urljoin(BASE_URL + \"/\", rel)\n",
    "                try:\n",
    "                    row, fecha_dt = scrap_articulo_requests(url_abs, filtrar_secciones=filtrar_secciones)\n",
    "                except Exception as e:\n",
    "                    logging.warning(\"Error en nota %s (%s/%s): %s\", url_abs, i, total, e)\n",
    "                    continue\n",
    "\n",
    "                if row:\n",
    "                    row[\"keyword\"] = kw\n",
    "                    resultados.append(row)\n",
    "\n",
    "                if i % 20 == 0 or i == total:\n",
    "                    logging.info(\"Notas procesadas: %s/%s\", i, total)\n",
    "\n",
    "    finally:\n",
    "        drv_scroll.quit()\n",
    "\n",
    "    df = pd.DataFrame(resultados)\n",
    "    if df.empty:\n",
    "        logging.warning(\"No se obtuvieron resultados nuevos.\")\n",
    "        return df\n",
    "\n",
    "    df.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "    save_incremental(df, out_path)\n",
    "    return df\n",
    "\n",
    "# ---------- RUN ----------\n",
    "if __name__ == \"__main__\" or True:\n",
    "    fecha_corte = datetime.strptime(FECHA_CORTE_STR, \"%Y-%m-%d\")\n",
    "\n",
    "    df_new = run_full(\n",
    "        candidatos=CANDIDATOS,\n",
    "        fecha_corte=fecha_corte,\n",
    "        out_path=OUT_PATH,\n",
    "        headless=HEADLESS,\n",
    "        max_notas_por_cand=MAX_NOTAS_POR_CAND,\n",
    "        filtrar_secciones=FILTRAR_SECCIONES\n",
    "    )\n",
    "\n",
    "    print(\"Nuevas filas:\", len(df_new))\n",
    "    try:\n",
    "        display(df_new.head())\n",
    "    except:\n",
    "        print(df_new.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
